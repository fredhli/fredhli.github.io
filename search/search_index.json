{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#fred-houze-li","title":"Fred Houze Li","text":""},{"location":"#lee-hou-tse","title":"\u674e\u539a\u6cfd   (Lee-Hou-Tse)","text":"<p>Welcome to my personal website! I am Fred, currently pursuing a Master's degree in Quantitative Finance at Washington University in St. Louis (WashU). Prior to that, I had a background in in Economics and Finance from The University of Hong Kong (HKU).</p> <p>I am passionate about data-driven financial analysis, quantitative capital market research, and integrating quantitative methods into political research. My interest spans programming, machine learning, and traditional financial tools.</p> <p>Feel free to explore my Projects, view my Resume, check my Sample Codes, or learn more about my Personal Life \u2013 including my travel photography collection and my cat \"\u5c0fi\" (Little-i)!</p>"},{"location":"#news","title":"News","text":"<ul> <li>November 1, 2024: Welcome to my new website!</li> </ul>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#my-resume","title":"My Resume","text":"<p> If not loaded properly, you can also download a PDF copy of my resume here. </p> <p></p>"},{"location":"mcmc/","title":"Sample Code 4","text":""},{"location":"mcmc/#sample-code-4","title":"Sample Code 4","text":"<p>In this individual practical project, I analyzed the decade-long performance of Vanguard ETFs using Bayesian methods and MCMC regression. The project aimed to evaluate CAPM, FF3, FF5, and FF6 models to identify the most effective model for factor-based pricing analysis. By employing the Chib and Zeng method, I selected predictive risk factors from six options, streamlining the analysis by focusing on essential factors. Additionally, I enhanced factor models with characteristics-based analysis to understand the impact of ETF characteristics on returns, comparing linear slope factors. The project involved managing complex datasets with R programming to ensure efficient data manipulation and preparation for comprehensive model analysis.</p>"},{"location":"mcmc/#codes","title":"Codes","text":""},{"location":"notion-sample/","title":"Sample Code 3","text":""},{"location":"notion-sample/#sample-code-3","title":"Sample Code 3","text":"<p>This Python script is a comprehensive automation toolkit that integrates with Notion API and web automation using Selenium. It includes utilities for PDF handling, text manipulation, and web scraping. Key features:</p> <ul> <li>Notion API integration for content management</li> <li>Selenium web automation with Chrome WebDriver</li> <li>PDF generation and manipulation</li> <li>Text parsing and formatting</li> <li>Automated web content extraction</li> </ul> <p>The code is organized into modular functions for better maintainability and reuse. Required dependencies include <code>selenium</code>, <code>notion-client</code>, <code>PyPDF2</code>, and other Python packages for web automation and document processing.</p>"},{"location":"notion-sample/#code","title":"Code","text":"<pre><code>import re\nimport os\nimport json\nimport time\nimport base64\nimport shutil\nimport PyPDF2\nimport requests\nimport pyperclip\nimport pdfplumber\nimport tkinter as tk\n\nfrom cv_info import *\nfrom config import notion_api_key\n\nfrom glob import glob\nfrom openai import OpenAI\nfrom PyPDF2 import PdfMerger\nfrom datetime import datetime\nfrom tkinter import messagebox\nfrom notion_client import Client\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.common.exceptions import ElementNotVisibleException\nfrom selenium.common.exceptions import ElementNotInteractableException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.webdriver.support import expected_conditions\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\n\n\ndef ready_for_chromedriver(binary_location, path_to_chromedriver):\n    # Specify some options for the webdriver to avoid exceptions\n    # Chromedriver: 113, Chrome: 113-32bits\n    options = webdriver.ChromeOptions()\n    options.binary_location = binary_location\n\n    # options.add_argument(\"start-maximized\")\n    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n    options.add_experimental_option(\"useAutomationExtension\", False)\n    prefs = {\n        \"credentials_enable_service\": False,\n        \"profile.password_manager_enabled\": False,\n        \"download.prompt_for_download\": False,\n        \"safebrowsing_for_trusted_sources_enabled\": False,\n        \"safebrowsing.enabled\": False,\n    }\n    options.add_experimental_option(\"prefs\", prefs)\n    # options.add_argument(\"--headless=new\")\n\n    path_to_chromedriver = path_to_chromedriver\n\n    # Initialize the webdriverchromedriver-win64\n    service = Service()\n\n    return path_to_chromedriver, service, options\n\n\ndef get_notion(api_key=notion_api_key):\n    return Client(auth=api_key)\n\n\ndef parse_text(extracted_text):\n    # Split by new lines\n    lines = extracted_text.split(\"\\n\")\n\n    result = []\n    for line in lines:\n        # Process bold text within each line\n        parts = re.split(r\"(\\*\\*.*?\\*\\*)\", line)\n\n        for part in parts:\n            if part.startswith(\"**\") and part.endswith(\"**\"):\n                result.append({\"content\": part[2:-2], \"bold\": True})\n            else:\n                result.append({\"content\": part, \"bold\": False})\n        # Add a line break (empty string block) after each line to simulate new line\n        result.append(\n            {\"content\": \"\\n\", \"bold\": False}\n        )  # Add this to represent a newline in Notion\n    # delete the last newline\n    del result[-1]\n    return result\n\n\ndef get_page_blocks(page_id):\n    notion = get_notion()\n    response = notion.blocks.children.list(block_id=page_id)\n    return response[\"results\"]\n\n\ndef find_block_by_text(blocks, search_text):\n    notion = get_notion()\n    for block in blocks:\n        if block[\"type\"] == \"paragraph\" and \"rich_text\" in block[\"paragraph\"]:\n            paragraph_text = \"\".join(\n                [t[\"text\"][\"content\"] for t in block[\"paragraph\"][\"rich_text\"]]\n            )\n            if search_text in paragraph_text:\n                return block[\"id\"], paragraph_text\n        elif block[\"type\"] == \"column_list\":\n            column_blocks = notion.blocks.children.list(\n                block_id=block[\"id\"])[\"results\"]\n            for column_block in column_blocks:\n                if column_block[\"type\"] == \"column\":\n                    column_content = notion.blocks.children.list(\n                        block_id=column_block[\"id\"]\n                    )[\"results\"]\n                    for sub_block in column_content:\n                        if (\n                            sub_block[\"type\"] == \"paragraph\"\n                            and \"rich_text\" in sub_block[\"paragraph\"]\n                        ):\n                            sub_paragraph_text = \"\".join(\n                                [\n                                    t[\"text\"][\"content\"]\n                                    for t in sub_block[\"paragraph\"][\"rich_text\"]\n                                ]\n                            )\n                            if search_text in sub_paragraph_text:\n                                return sub_block[\"id\"], sub_paragraph_text\n    return None, None\n\n\ndef update_block_content(block_id, text_segments, change_color=False, color_to=None):\n    notion = get_notion()\n    rich_text = [\n        {\n            \"type\": \"text\",\n            \"text\": {\"content\": segment[\"content\"]},\n            \"annotations\": {\"bold\": segment.get(\"bold\", False)},\n        }\n        for segment in text_segments\n    ]\n\n    if not change_color:\n        notion.blocks.update(block_id=block_id, paragraph={\n                             \"rich_text\": rich_text})\n        time.sleep(2)\n        print(f\"Block {block_id} updated.\")\n    else:\n        notion.blocks.update(\n            block_id=block_id, paragraph={\n                \"rich_text\": rich_text, \"color\": color_to}\n        )\n        time.sleep(2)\n        print(f\"Block {block_id} updated with color '{color_to}'.\")\n\n\ndef replace_simple(old_text, new_text, page_id, change_color=False, color_to=None):\n    new_text = parse_text(new_text)\n\n    blocks = get_page_blocks(page_id)\n    block_id, found_text = find_block_by_text(blocks, old_text)\n\n    if block_id:\n        print(f\"Found block with text: {found_text}\")\n        update_block_content(block_id, new_text, change_color, color_to)\n    else:\n        assert False, f\"Text '{old_text}' not found in the page.\"\n\n    return True, block_id\n\n\ndef check_TBA(driver):\n    retry = 0\n    while retry &lt;= 3:\n        try:\n            body_text = driver.find_element(By.TAG_NAME, \"body\").text\n            if \"TBA\" in body_text:\n                print(f\"TBA found on retry {retry}, refreshing the page...\")\n                driver.refresh()\n                time.sleep(3)\n                retry += 1\n            else:\n                print(\"No TBA found, page is successful.\")\n                return True\n        except NoSuchElementException:\n            raise Exception(\"Unable to find page body element.\")\n\n    body_text = driver.find_element(By.TAG_NAME, \"body\").text\n    if \"TBA\" in body_text:\n        raise Exception(\n            \"Page contains 'TBA' after 3 retries, indicating an issue.\")\n    else:\n        print(\"No TBA found after retries.\")\n        return True\n\n\ndef print_to_pdf(driver, file_path):\n    print_options = {\n        \"scale\": 0.83,  # 85% margin\n        \"paperWidth\": 8.27,  # A4 width\n        \"paperHeight\": 11.69,  # A4 height\n        # 'marginTop': 0.4,\n        # 'marginBottom': 0.4,\n        # 'marginLeft': 0.4,\n        # 'marginRight': 0.4,\n        # 'printBackground': True,     # Print background graphics\n        \"preferCSSPageSize\": True,  # Enable CSS page size preference\n    }\n\n    result = driver.execute_cdp_cmd(\"Page.printToPDF\", print_options)\n\n    with open(file_path, \"wb\") as f:\n        f.write(base64.b64decode(result[\"data\"]))\n\n    print(f\"PDF saved to {file_path}\")\n\n\ndef selenium_download_pdf(service, options, download_folder, url):\n    driver = webdriver.Chrome(service=service, options=options)\n    driver.get(url)\n    time.sleep(5)\n    driver.refresh()\n    time.sleep(5)\n\n    if check_TBA(driver):\n        print(\"Page is successful, proceeding to download PDF.\")\n\n    print_to_pdf(driver, f\"{download_folder}/Cover Letter - Fred Li.pdf\")\n    driver.quit()\n    return True\n\n\ndef remove_blank_pages_from_pdf(input_pdf_path):\n    with open(input_pdf_path, \"rb\") as file:\n        pdf = PyPDF2.PdfReader(file)\n\n        if len(pdf.pages) &lt;= 1:\n            print(\"Only one page in the PDF. No need to remove blank pages.\")\n            return\n\n        with pdfplumber.open(input_pdf_path) as pdf_reader:\n            first_page_text = pdf_reader.pages[0].extract_text()\n\n            pages_to_keep = 1\n            for i in range(1, len(pdf_reader.pages)):\n                page_text = pdf_reader.pages[i].extract_text()\n\n                if page_text and page_text.strip():\n                    pages_to_keep = i + 1\n                else:\n                    print(f\"Page {i + 1} is blank. Removing...\")\n                    break\n\n        pdf_writer = PyPDF2.PdfWriter()\n\n        with open(input_pdf_path, \"rb\") as file:\n            pdf = PyPDF2.PdfReader(file)\n            for page_num in range(pages_to_keep):\n                pdf_writer.add_page(pdf.pages[page_num])\n\n        with open(input_pdf_path, \"wb\") as output_file:\n            pdf_writer.write(output_file)\n\n        print(f\"Saved PDF with {pages_to_keep} non-blank pages.\")\n\n\ndef replace_text_in_blocks(blocks, start_text, end_text, replacement_text):\n    in_replacement_section = False\n    replacement_done = False\n\n    notion = get_notion()\n    for block in blocks:\n        if block[\"type\"] == \"paragraph\" and \"rich_text\" in block[\"paragraph\"]:\n            paragraph_text = \"\".join(\n                [t[\"text\"][\"content\"] for t in block[\"paragraph\"][\"rich_text\"]]\n            )\n\n            if start_text in paragraph_text:\n                in_replacement_section = True\n                continue\n\n            if end_text in paragraph_text and in_replacement_section:\n                in_replacement_section = False\n                replacement_done = True\n                continue\n\n            if in_replacement_section:\n                update_block_content_2(block[\"id\"], \"\")\n                update_block_content_2(block[\"id\"], replacement_text)\n                replacement_done = True\n\n        if block.get(\"has_children\", False):\n            child_blocks = notion.blocks.children.list(\n                block_id=block[\"id\"])[\"results\"]\n            replacement_done = (\n                replace_text_in_blocks(\n                    child_blocks, start_text, end_text, replacement_text\n                )\n                or replacement_done\n            )\n\n    return replacement_done\n\n\ndef update_block_content_2(block_id, new_text):\n    notion = get_notion()\n    notion.blocks.update(\n        block_id=block_id,\n        paragraph={\"rich_text\": [\n            {\"type\": \"text\", \"text\": {\"content\": new_text}}]},\n    )\n    print(f\"Updated block {block_id} with new text.\")\n    time.sleep(1)\n</code></pre>"},{"location":"personal/","title":"Personal","text":""},{"location":"personal/#personal","title":"Personal","text":""},{"location":"personal/#photo-collection","title":"Photo Collection","text":"Seine River, Paris, France Big Sur, California, USA St. Louis, Missouri, USA in Winter Grand Canyon, Arizona, USA Train from Kandy to Colombo, Sri Lanka Windermere, Lake District, UK K\u00fcssnacht am Rigi, Switzerland G\u00f6reme, T\u00fcrkiye Istanbul, T\u00fcrkiye Chiang Mai, Thailand Border Restriction Area between Hong Kong and Shenzhen Paris, France Berlin, Germany Hong Kong Prague, Czech Republic Paris, France Amman, Jordan Budapest, Hungary Lapland, Finland Wengen, Switzerland        Your browser does not support the video tag.      Children at a monastery in Inle Lake, Myanmar (Burma) Western Wall, Jerusalem, two days before the United States moved its embassy from Tel Aviv to Jerusalem Sommar\u00f8y, Norway Troms\u00f8, Norway"},{"location":"personal/#cat-collection","title":"Cat Collection","text":"Dec 15, 2023 Mar 12, 2024 May 05, 2024 Oct 06, 2024, \u5c0fi with \u74dc\u5b50 (Melon) Nov 01, 2024, \u5c0fi is teaching me how to build up my website and upload photos of him"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#projects","title":"Projects","text":"<p>Here are my ongoing and completed projects:</p>"},{"location":"projects/#price-prediction-prop-trading-strategy-just-started","title":"Price Prediction Prop Trading Strategy (Just Started)","text":"<ul> <li>GitHub Repository TBA</li> <li>This project involves building a machine learning model to predict stock prices using historical data. The project leverages various statistical techniques and data preprocessing methods to enhance prediction accuracy.</li> </ul>"},{"location":"projects/#hull-white-model-calibration","title":"Hull-White Model Calibration","text":""},{"location":"projects/#description","title":"Description","text":"<p>This project focuses on the intricate task of calibrating the Hull-White model to at-the-money (ATM) caplet market implied volatilities. Through a meticulous process involving both theoretical and simulation-based approaches, we bridged the gap between model predictions and market observations. This project delves into the dynamics of fixed income derivatives, exploring the market movements of the Hull-White model and its practical applications in today's financial markets.</p>"},{"location":"projects/#methodology","title":"Methodology","text":"<p>To increase the sample size, we combined different caplets to obtain 120 caps from 120 caplets, spanning various maturities, for the calibration process. The calibration involved solving an optimization problem where the objective function was to minimize the sum of squared differences between the model-implied volatilities and the observed market volatilities. Additionally, we employed Monte-Carlo Simulation methods to make our pricing model accurate, and to ensuring robustness and reliability of the results.</p> <p></p>"},{"location":"projects/#output","title":"Output","text":"<p>Our calibration results for Hull-White short-rate model:</p> \\[ \\begin{aligned} &amp;dr(t) = \\theta(t) - 0.01335 \\cdot r(t) \\, dt + 0.02013 \\cdot dW(t) \\\\ \\\\ &amp;\\text{where,} \\\\ &amp;\\theta(t) = \\frac{\\partial f(0,t)}{\\partial t} + 0.01335 \\cdot f(0,t) + \\frac{\\sigma^2}{2 0.01335} \\left(1 - e^{-2 \\times 0.01335 t}\\right) \\\\ &amp;r(0) = 0.04815 \\end{aligned} \\]"},{"location":"projects/#plot","title":"Plot","text":"<p>The calibration resulted in an around 7% loss in function value.</p> <p></p>"},{"location":"projects/#automatic-cover-letter-generator","title":"Automatic Cover Letter Generator","text":"<p>This project focuses on creating an automatic cover letter generator using natural language processing techniques.</p>"},{"location":"projects/#description_1","title":"Description","text":"<p>This project automates the process of using AI API to generate well-organized, highly-tailored cover letters during your job hunt. It allows you to manage multiple versions of your CV and lets AI help you choose which CV to use based on the required skillsets in job descriptions.</p>"},{"location":"projects/#features","title":"Features","text":"<ul> <li>CV Management: Store and manage multiple versions of your CV.</li> <li>JD Analysis: Extract key requirements and skills from job descriptions.</li> <li>Company Address Search Up: Use Google Map API to automate the company address hunt process.</li> <li>CV-JD Matching: AI-powered matching of CV versions to job requirements.</li> <li>Cover Letter Generation: Automated creation of personalized cover letters using Notion API.</li> <li>API Integration: Seamless interaction with ChatGPT API.</li> <li>Configuration Options: Customize output format and style preferences.</li> <li>Centralized Storage of Application History: Keep track of all your job applications and generated cover letters in one place for easy reference and follow-up.</li> </ul>"},{"location":"research_experience/","title":"Research Experience","text":""},{"location":"research_experience/#research-experience","title":"Research Experience","text":"<p>Here is a summary of my contributions to ongoing research projects.</p>"},{"location":"research_experience/#text-based-analysis-using-novel-dataset","title":"Text-based Analysis using Novel Dataset","text":"<p>We used a novel dataset of U.S. congressional hearings. After rigorously addressing OCR reading errors and mapping database speakers to real-world congressmen, our next step is to train a BERT topic model to condense hundreds of thousands of speeches in the dataset into 500 topics. </p>"},{"location":"research_experience/#plot-1","title":"Plot 1","text":"<p>Following fine-tuning (including embedding pretraining, vectorizer model, UMAP model, HDBSCAN model, c-TF-IDF model, and representation model), this is an interactive map of the top 20 most frequent topics. Feel free to explore the data:</p>"},{"location":"research_experience/#plot-2","title":"Plot 2","text":"<p>Here is another plot which depicits the frequency of how congressmen mentions a certain redacted label over time, labelled by AI-Large Language Models (LLMs). </p> <p></p>"},{"location":"research_experience/#the-casual-impact-of-fiscal-shock","title":"The Casual Impact of Fiscal Shock","text":"<p>This research investigates how government deficits and public debt levels influence asset prices and macroeconomic conditions. Using event study methodology, we analyze periods where UK budget deficit news emerged independently of economic conditions, addressing the omitted variable bias challenge (OVB). We employ Large Language Models (LLMs) to extract budget surprises from news data and examine their impact on financial markets.</p> <p>We obtained <code>q1_ratio</code> by utilizing an AI LLMs to assess news articles related to budget announcements. For each article, the model answered two specific questions about whether the news would cause the budget deficit to increase or decrease. The responses were categorized as 'up', 'down', or 'unsure'. The <code>q1_ratio</code> is calculated as a normalized measure using the proportions of 'up' and 'down' responses:</p> \\[\\text{q1_ratio} = \\frac{\\text{q1_up} - \\text{q1_down}}{\\text{q1_up} + \\text{q1_down}}\\] <p>Then, we conducted following regression analysis:</p> <pre><code>q1_ratio ~ 10.0_n + gbpusd_o\n</code></pre> <ul> <li> <p>10.0_n: change in UK 10-year bond yield between announcement date <code>t+5</code> and <code>t-1</code> </p> </li> <li> <p>gbpusd_o: change in GBP/USD Exchange Rate between announcement date <code>t+5</code> and <code>t-1</code> </p> </li> </ul> <p>Here are the preliminary regression results:</p> <pre><code>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 10.0_n   R-squared:                       0.399\nModel:                            OLS   Adj. R-squared:                  0.373\nMethod:                 Least Squares   F-statistic:                     15.25\nDate:                Sat, 02 Nov 2024   Prob (F-statistic):           1.03e-07\nTime:                        23:57:16   Log-Likelihood:                 45.267\nNo. Observations:                  73   AIC:                            -82.53\nDf Residuals:                      69   BIC:                            -73.37\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst           0.0379      0.021      1.782      0.079      -0.005       0.080\nq1_ratio        0.1712      0.050      3.417   0.001***       0.071       0.271\ngbpeur_o       -5.7955      1.356     -4.273   0.000***      -8.501      -3.090\ninteraction   -11.7326      2.721     -4.312   0.000***     -17.161      -6.304\n==============================================================================\nOmnibus:                        0.705   Durbin-Watson:                   1.541\nProb(Omnibus):                  0.703   Jarque-Bera (JB):                0.228\nSkew:                           0.022   Prob(JB):                        0.892\nKurtosis:                       3.270   Cond. No.                         186.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</code></pre> <p>The regression results indicate a significant positive relationship between q1_ratio and the change in UK ten-year government bond yields (10.0_n). Specifically, a higher <code>q1_ratio</code> \u2014 implying stronger public expectation of an increasing budget deficit \u2014 is associated with an increase in bond yields. The coefficient for q1_ratio is positive and statistically significant at the 1% level. Additionally, the negative coefficients for the change in the GBP/EUR exchange rate (gbpeur_o) and the interaction term suggest that currency appreciation and its interplay with budget expectations also influence bond yields. These findings support the hypothesis that anticipated fiscal expansions lead investors to demand higher yields on government bonds, reflecting the pricing-in of budget deficit expectations into asset prices.</p>"},{"location":"sample_codes/","title":"Sample Codes","text":""},{"location":"sample_codes/#sample-codes","title":"Sample Codes","text":"<p>Welcome to my sample codes page! Here you'll find a collection of code snippets and projects that showcase my technical skills.</p> <p>Feel free to explore and learn from these implementations. Each piece of code is documented to help you understand the underlying concepts and methodologies.</p>"},{"location":"sample_codes/#sample-code-1","title":"Sample Code 1","text":""},{"location":"sample_codes/#text-based-analysis-of-novel-dataset","title":"Text-based Analysis of Novel Dataset","text":"<p>This Python script is a comprehensive text analysis tool designed for processing and analyzing a novel congressional hearing dataset. The code implements multiple natural language processing techniques for analyzing complex document structures and congressional discussions.</p> <ul> <li>Text Analysis: Advanced NLP techniques and document processing</li> <li>Sentiment Analysis: Emotion and opinion detection in congressional discussions</li> <li>OCR Error Correction: Automated text correction and enhancement</li> <li>Speaker Identification: Recognition and matching of committee members</li> <li>Entity Recognition: Using SpaCy and Hugging Face transformers</li> <li>Document Parsing: Complex structure analysis and text preprocessing</li> </ul>"},{"location":"sample_codes/#sample-code-2","title":"Sample Code 2","text":""},{"location":"sample_codes/#tf-idf-matching","title":"TF-IDF Matching","text":"<p>This Python script demonstrates text similarity matching using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity. The code implements efficient blocking and matching functions for comparing large sets of company names or similar text data.</p> <ul> <li>Text Processing: N-gram generation and tokenization</li> <li>Similarity Calculation: Optimized cosine similarity using sparse matrices</li> <li>TF-IDF Vectorization: Document feature extraction</li> <li>Efficient Blocking: Strategic data partitioning for large datasets</li> </ul>"},{"location":"sample_codes/#sample-code-3","title":"Sample Code 3","text":""},{"location":"sample_codes/#selenium-notion-pdf-scraper","title":"Selenium-Notion PDF Scraper","text":"<p>This Python script is a comprehensive automation toolkit that integrates with Notion API and web automation using Selenium. It includes utilities for PDF handling, text manipulation, and web scraping. Key features:</p> <ul> <li>Notion API integration for content management</li> <li>Selenium web automation with Chrome WebDriver</li> <li>PDF generation and manipulation</li> <li>Text parsing and formatting</li> <li>Automated web content extraction</li> </ul>"},{"location":"text-based-codes/","title":"Sample Code 1","text":""},{"location":"text-based-codes/#sample-code-1","title":"Sample Code 1","text":""},{"location":"text-based-codes/#text-based-analysis-of-novel-dataset","title":"Text-based Analysis of Novel Dataset","text":"<p>This Python script is a comprehensive text analysis tool designed for processing and analyzing congressional hearing transcripts. It combines multiple natural language processing techniques, including sentiment analysis, OCR error correction, speaker identification, and committee member matching. The script utilizes various libraries such as spaCy, pandas, and the Hugging Face transformers library to handle tasks ranging from basic text preprocessing to advanced entity recognition. Key features include the ability to parse complex document structures, match speakers with committee members, correct OCR errors, and analyze sentiment in congressional discussions.</p>"},{"location":"text-based-codes/#code","title":"Code","text":"<pre><code>import re\nimport spacy\nimport numpy as np\nimport pandas as pd\n\nfrom os.path import join, basename\nfrom glob import glob\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-classification\",\n    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n    max_length=512,\n    truncation=True,\n)\n\nroot = \"C:/Users/lihou/Box/[Redacted]\"\nmetadata = join(root, \"data\", \"ProQuest\", \"processed_metadata\")\n\n# maybe this is a better idea\nchairman_ocr_errors = [\n    \"The CHAIRMAN\",\n    \"The CIIAIRMM\",\n    \"The CHAIRMAx\",\n    \"The CHAIRMINAN\",\n    # Omitted\n]\n\n# -------------------------------------------------------------------------------------------------\n# Functions\n# -------------------------------------------------------------------------------------------------\n\n\ndef get_correct_txt_from_folder(folder, us_states):\n    \"\"\"\n    Identifies the text file in a folder that most likely contains a name list by analyzing state references.\n\n    Args:\n        folder (str): Path to the folder containing text files\n        us_states (list): List of US state names/abbreviations to check for\n\n    Returns:\n        str: Name of the file with highest state-to-line ratio, or None if no files found\n    \"\"\"\n    txt_files = glob(join(folder, \"*.txt\"))\n    if not txt_files:\n        return None\n\n    result_files = {}\n\n    for txt_file in txt_files:\n        with open(txt_file, 'r') as f:\n            lines = f.readlines()\n            content = ''.join(lines)\n            line_count = len(lines)\n\n            if line_count == 0:\n                continue\n\n            state_count = sum(content.count(state) for state in us_states)\n            file_name = basename(txt_file)\n            result_files[file_name] = state_count / line_count\n\n    return max(result_files, key=result_files.get) if result_files else None\n\n\ndef get_start_indices(txt):\n    \"\"\"\n    Identifies starting indices for main committee and subcommittees in a text file.\n\n    Args:\n        txt (str): Path to the text file to analyze\n\n    Returns:\n        tuple: (main_committee_index, subcommittee_indices_dict)\n            - main_committee_index (int): Line index where main committee starts\n            - subcommittee_indices_dict (dict): Dictionary mapping subcommittee numbers to their starting line indices\n    \"\"\"\n    main_start = None\n    subcommittee_starts = {}\n    subcommittee_count = 0\n\n    with open(txt, \"r\") as f:\n        for i, line in enumerate(f):\n            words = line.split()\n\n            # Skip empty lines\n            if not words:\n                continue\n\n            # Check for main committee\n            if main_start is None and any(fuzz.ratio(word, \"COMMITTEE\") &gt; 70 for word in words):\n                main_start = i\n                continue\n\n            # Check for subcommittees (only after main committee is found)\n            if main_start is not None and any(fuzz.ratio(word, \"SUBCOMMITTEE\") &gt; 70 for word in words):\n                subcommittee_starts[subcommittee_count] = i\n                subcommittee_count += 1\n\n    return main_start, subcommittee_starts\n\n\ndef parse_line(line, suffix_list, us_states, identity_list):\n    \"\"\"\n    Parse a line containing speaker information in various formats and return a DataFrame.\n\n    Handles the following formats:\n    1. Normal: 'SPEAKER, State' or 'SPEAKER, Identity'\n    2. Both: 'SPEAKER, State, Identity'\n    3. Line change errors with mixed capitalization\n    4. Suffix variations\n\n    Args:\n        line (str): Input line to parse\n        suffix_list (list): List of valid suffixes\n        us_states (list): List of US state names/abbreviations\n        identity_list (list): List of known identities\n\n    Returns:\n        pd.DataFrame: DataFrame with columns [Speaker, State, Identity, Suffix, Original Line]\n    \"\"\"\n    dataframe = pd.DataFrame(columns=[\"Speaker\", \"State\", \"Identity\", \"Suffix\", \"Original Line\"])\n    updated_identity_list = identity_list.copy()\n\n    # Clean and prepare the line\n    line = line.replace(\"\\\\n\", \"\")\n    updated_lines = split_line_on_pattern(line)\n\n    # Process each line after potential splitting\n    for updated_line in updated_lines:\n        # Parse parts and handle suffix\n        parts = [part.strip() for part in updated_line.split(\",\")]\n        parts, suffix = extract_suffix(parts, suffix_list)\n\n        if not is_valid_name_length(parts):\n            continue\n\n        # Process based on number of parts\n        if len(parts) == 1:\n            row = create_single_part_row(parts[0], suffix, line)\n        elif len(parts) == 2:\n            row = create_double_part_row(parts[0], parts[1], suffix, line, us_states, updated_identity_list)\n        elif len(parts) == 3:\n            row = create_triple_part_row(parts, suffix, line, us_states, updated_identity_list)\n        else:\n            continue\n\n        if row is not None:\n            dataframe = pd.concat([dataframe, pd.DataFrame([row])], ignore_index=True)\n\n    return dataframe\n\ndef split_line_on_pattern(line):\n    \"\"\"Split line on pattern indicating missing newlines.\"\"\"\n    pattern = re.compile(r\"([A-Z][a-z]+) ([A-Z][A-Z]+)\")\n    parts = line.split(\", \")\n    updated_lines = []\n\n    for i, part in enumerate(parts):\n        match = pattern.search(part)\n        if match:\n            new_parts = pattern.sub(r\"\\1\\n\\2\", part).split(\"\\n\")\n            if i &gt; 0:\n                updated_lines.append(f\"{', '.join(parts[:i])}, {new_parts[0]}\")\n            else:\n                updated_lines.append(new_parts[0])\n\n            if i &lt; len(parts) - 1:\n                updated_lines.append(f\"{new_parts[1]}, {', '.join(parts[i + 1:])}\")\n            else:\n                updated_lines.append(new_parts[1])\n            return updated_lines\n\n    return [line]\n\ndef extract_suffix(parts, suffix_list):\n    \"\"\"Extract suffix from parts if present.\"\"\"\n    suffix = None\n    parts_no_suffix = []\n\n    for part in parts:\n        if part in suffix_list:\n            suffix = part\n        else:\n            parts_no_suffix.append(part)\n\n    return parts_no_suffix, suffix\n\ndef is_valid_name_length(parts):\n    \"\"\"Check if name length is valid.\"\"\"\n    return not (parts and len(parts[0].split()) &gt; 5)\n\ndef create_single_part_row(speaker, suffix, original_line):\n    \"\"\"Create row for single part entry.\"\"\"\n    return {\n        \"Speaker\": speaker,\n        \"State\": None,\n        \"Identity\": None,\n        \"Suffix\": suffix,\n        \"Original Line\": original_line\n    }\n\ndef create_double_part_row(speaker, second_part, suffix, original_line, us_states, identity_list):\n    \"\"\"Create row for double part entry.\"\"\"\n    state = process.extractOne(second_part, us_states, score_cutoff=65)\n\n    if state is not None:\n        return {\n            \"Speaker\": speaker,\n            \"State\": state[0],\n            \"Identity\": None,\n            \"Suffix\": suffix,\n            \"Original Line\": original_line\n        }\n    else:\n        if not any(second_part in identity for identity in identity_list):\n            identity_list.append(second_part)\n        return {\n            \"Speaker\": speaker,\n            \"State\": None,\n            \"Identity\": second_part,\n            \"Suffix\": suffix,\n            \"Original Line\": original_line\n        }\n\ndef create_triple_part_row(parts, suffix, original_line, us_states, identity_list):\n    \"\"\"Create row for triple part entry.\"\"\"\n    speaker, part2, part3 = parts\n    state_part2 = process.extractOne(part2, us_states, score_cutoff=65)\n    state_part3 = process.extractOne(part3, us_states, score_cutoff=65)\n\n    if state_part2 is not None and state_part3 is None:\n        if not any(part3 in identity for identity in identity_list):\n            identity_list.append(part3)\n        return {\n            \"Speaker\": speaker,\n            \"State\": state_part2[0],\n            \"Identity\": part3,\n            \"Suffix\": suffix,\n            \"Original Line\": original_line\n        }\n    elif state_part3 is not None and state_part2 is None:\n        if not any(part2 in identity for identity in identity_list):\n            identity_list.append(part2)\n        return {\n            \"Speaker\": speaker,\n            \"State\": state_part3[0],\n            \"Identity\": part2,\n            \"Suffix\": suffix,\n            \"Original Line\": original_line\n        }\n    return None\n\n\ndef generate_ranges(main_index, sub_indices, total_lines):\n    \"\"\"\n    This function serves to generate line indices based on the main_index and sub_indices.\n    \"\"\"\n    subs = []\n    if main_index is None:\n        return []\n\n    # Filter sub_indices to remove main_index\n    else:\n        for i in range(len(sub_indices)):\n            if sub_indices[i] != main_index:\n                subs.append(sub_indices[i])\n\n    if len(subs) == 0:\n        return [(main_index, total_lines)]\n    else:\n        subs.sort()\n        # return (main_index, subs[0]), (subs[0], subs[1]), ..., (subs[-1], total_lines)\n        ranges = [(main_index, subs[0])]\n        for i in range(len(subs) - 1):\n            ranges.append((subs[i], subs[i + 1]))\n        ranges.append((subs[-1], total_lines))\n        return ranges\n\n\ndef extract_committee(txt, suffix_list, us_states, identity_list):\n    \"\"\"\n    Extract potential speakers from committee and subcommittee sections in a text file.\n\n    Args:\n        txt (str): Path to the text file to analyze.\n        suffix_list (list): List of valid suffixes.\n        us_states (list): List of US state names/abbreviations.\n        identity_list (list): List of known identities.\n\n    Returns:\n        pd.DataFrame: DataFrame containing speaker information.\n    \"\"\"\n    file_name = basename(txt)\n    year = int(re.findall(r\"\\d{4}\", file_name)[0])\n\n    dataframe = pd.DataFrame(\n        columns=[\n            \"Speaker\", \"State\", \"Identity\", \"Suffix\", \"Original Line\",\n            \"Committee\", \"File Name\", \"Year\"\n        ]\n    )\n    main_index, sub_indices = get_start_indices(txt)\n\n    with open(txt, \"r\") as f:\n        lines = f.readlines()\n        total_lines = len(lines)\n        ranges = generate_ranges(main_index, sub_indices, total_lines)\n\n        if not ranges:\n            return dataframe\n\n        for start, end in ranges:\n            end = min(end, start + 50)\n            committee = None\n\n            for i, line in enumerate(lines[start:end], start=start):\n                if i == start:\n                    committee = line.lower()\n                else:\n                    df_to_concat = parse_line(line, suffix_list, us_states, identity_list)\n                    df_to_concat[\"Committee\"] = committee\n                    df_to_concat[\"File Name\"] = file_name\n                    df_to_concat[\"Year\"] = year\n                    dataframe = pd.concat([dataframe, df_to_concat], ignore_index=True)\n\n    return dataframe\n\n\n\ndef correct_identity(identity, acceptable_identity_list):\n    \"\"\"\n    Corrects the identity of a speaker based on an acceptable identity list.\n\n    Args:\n        identity (str): The identity to be corrected.\n        acceptable_identity_list (list): List of acceptable identities.\n\n    Returns:\n        str: Corrected identity or 'no_identity'/'no_match' if not found.\n    \"\"\"\n    if not identity or identity in [None, \".\", \"\"]:\n        return \"no_identity\"\n\n    identity = identity.lower()\n    result = process.extractOne(identity, acceptable_identity_list, score_cutoff=70)\n\n    return result[0] if result else \"no_match\"\n\n\n\ndef estimate_individual_probability(name, entity_type, nlp):\n    templates = [\n        f\"During the hearing, {name} provided expert testimony on the matter.\",\n        f\"{name}, a recognized authority in the field, was cited frequently during the discussions.\",\n        f\"The statement by {name} was referenced multiple times by committee members.\",\n        f\"Senator Johnson addressed {name} directly during the questioning session.\",\n        f\"As noted in the proceedings, {name} has been a key figure in this investigation.\",\n        f\"{name}'s written testimony, submitted prior to the session, outlined several key points.\",\n        f\"The amendment proposed by {name} was considered during the debate.\",\n        f\"Congresswoman Smith thanked {name} for their detailed report on the issue.\",\n        f\"The chairperson asked {name} to clarify their previous statements.\",\n        f\"According to {name}, the data presented during the hearing was conclusive.\",\n        f\"In his closing remarks, the attorney general mentioned {name}'s contributions to the case.\",\n        f\"The recent policy paper by {name} was mentioned as a significant piece of evidence.\",\n        f\"During the panel discussion, {name} argued for stricter regulations.\",\n        f\"The insights from {name} were pivotal to the committee's understanding of the issue.\",\n        f\"After the session, {name} was interviewed by several media outlets regarding their stance.\",\n        f\"{name} was among the experts invited to discuss the implications of the new legislation.\",\n        f\"The findings from {name}'s research were heavily debated during the hearing.\",\n        f\"In an unexpected move, {name} challenged the committee's previous conclusions.\",\n        f\"{name} was acknowledged by the chair for their extensive work on the subject.\",\n        f\"Following the recommendations by {name}, the committee decided to revise their initial position.\",\n        f\"{name}'s previous work on the subject was highly regarded by the committee members.\",\n        f\"During the testimony, {name} cited several recent studies to support their argument.\",\n        f\"The historical context provided by {name} was essential for understanding the debate.\",\n        f\"Representative Davis highlighted {name}'s contributions to the field during her speech.\",\n        f\"{name}, a long-standing member of the board, offered a dissenting opinion.\",\n        f\"The policy brief authored by {name} was circulated among the attendees.\",\n        f\"In his memo, {name} outlined the potential impacts of the proposed changes.\",\n        f\"{name} was called upon to further elaborate on their previous remarks.\",\n        f\"The committee praised {name} for their meticulous research and presentation skills.\",\n        f\"{name} responded to the queries regarding the accuracy of the data presented.\",\n        f\"Several participants expressed their support for the initiatives suggested by {name}.\",\n        f\"The legal perspectives shared by {name} were crucial to the discussions.\",\n        f\"In their written submission, {name} addressed several critical issues.\",\n        f\"{name}'s role as a mediator was commended by all parties involved.\",\n        f\"The detailed analysis by {name} helped shape the committee's final decision.\",\n        f\"During the recess, {name} discussed the implications of the hearing with the press.\",\n        f\"The theoretical framework developed by {name} was a focal point of the session.\",\n        f\"{name}'s credentials were verified by the committee before the hearing.\",\n        f\"The cross-examination by {name} brought new facts to light.\",\n        f\"{name} was responsible for compiling the comprehensive report on the issue.\",\n    ]\n\n    # Count the occurrences where the name is recognized as the specified entity type\n    count = 0\n    total = 0\n    for template in templates:\n        doc = nlp(template)\n        for ent in doc.ents:\n            if ent.text == name and ent.label_ == entity_type:\n                count += 1\n        total += 1\n\n    # Return the probability of the name being recognized as the specified entity type\n    return count / total if total &gt; 0 else 0\n\n\ndef match_committee_with_pol(row, congress):\n    \"\"\"\n    This function serves to match the committee names with an existing house and senate database to correct any misspellings.\n    \"\"\"\n    name = str(row[\"committee_member\"])\n    name = name.replace(\".\", \"\")\n    year = int(row[\"year\"])\n    state = row[\"state_po\"]\n    # to prevent any errors in North/South,East/West\n    if state == \"NC\" or state == \"SC\":\n        state = \"CAR\"\n    if state == \"ND\" or state == \"SD\":\n        state = \"DAK\"\n    if state == \"VA\" or state == \"WV\":\n        state = \"VIR\"\n    if state == \"WA\" or state == \"DC\":\n        state = \"WAS\"\n\n    if pd.isna(name) or name == \"\" or name == \".\" or name == \"nan\" or name is None:\n        row[\"matched_name\"] = \"no_match\"\n        row[\"matched_party\"] = \"no_match\"\n        row[\"matched_office\"] = \"no_match\"\n        row[\"matched_year\"] = \"no_match\"\n        row[\"matched_method\"] = \"no_match\"\n        row[\"matched_score\"] = \"no_match\"\n        row[\"matched_state\"] = \"no_match\"\n        row[\"without_state_and_match\"] = False\n        return row\n\n    # For a house member, range: (year - 2, year); for a senate member, range: (year - 6, year)\n    house_range = range(year - 2, year + 1)\n    senate_range = range(year - 6, year + 1)\n\n    house_criteria = (congress[\"office\"] == \"house\") &amp; (\n        congress[\"year\"].isin(house_range)\n    )\n    senate_criteria = (congress[\"office\"] == \"senate\") &amp; (\n        congress[\"year\"].isin(senate_range)\n    )\n\n    congress_possible = congress[house_criteria | senate_criteria].copy()\n\n    if len(congress_possible) == 0:\n        print(f\"No matches in {year}\")\n        row[\"matched_name\"] = \"no_match\"\n        row[\"matched_party\"] = \"no_match\"\n        row[\"matched_office\"] = \"no_match\"\n        row[\"matched_year\"] = \"no_match\"\n        row[\"matched_method\"] = \"no_match\"\n        row[\"matched_score\"] = \"no_match\"\n        return row\n\n    # STEP 1: MATCH FULL NAME\n    # Match the name with the congress_possible\n    possible_names = congress_possible[\"candidate\"].tolist()\n    results = process.extract(name, possible_names)\n    # Only keep results with score &gt; 80\n    results = [result for result in results if result[1] &gt; 80]\n    congress_match_fullname = congress_possible[\n        congress_possible[\"candidate\"].isin([result[0] for result in results])\n    ]\n    if pd.isna(state) and len(congress_match_fullname) &gt;= 1:\n        row[\"matched_name\"] = congress_match_fullname[\"candidate\"].values[0]\n        row[\"matched_party\"] = congress_match_fullname[\"party\"].values[0]\n        row[\"matched_office\"] = congress_match_fullname[\"office\"].values[0]\n        row[\"matched_year\"] = congress_match_fullname[\"year\"].values[0]\n        row[\"matched_method\"] = \"full_name\"\n        row[\"matched_score\"] = results[0][1]\n        row[\"matched_state\"] = congress_match_fullname[\"state_po_original\"].values[0]\n        row[\"without_state_and_match\"] = True\n        return row\n\n    else:\n        # Check the state\n        congress_match_fullname = congress_match_fullname[\n            congress_match_fullname[\"state_po\"] == state\n        ]\n        if len(congress_match_fullname) &gt;= 1:\n            row[\"matched_name\"] = congress_match_fullname[\"candidate\"].values[0]\n            row[\"matched_party\"] = congress_match_fullname[\"party\"].values[0]\n            row[\"matched_office\"] = congress_match_fullname[\"office\"].values[0]\n            row[\"matched_year\"] = congress_match_fullname[\"year\"].values[0]\n            row[\"matched_method\"] = \"full_name\"\n            row[\"matched_score\"] = results[0][1]\n            row[\"matched_state\"] = congress_match_fullname[\"state_po_original\"].values[\n                0\n            ]\n            row[\"without_state_and_match\"] = False\n            return row\n\n        # STEP 2 &amp; 3: MATCH LAST NAME AND FIRST NAME IF POSSIBLE. IF NOT, ONLY MATCH LAST NAME\n        else:\n            last_name = row[\"last_name\"]\n            if pd.isna(last_name) or last_name == \"\" or last_name == \".\":\n                print(f\"No matches: {name} in {year}\")\n                row[\"matched_name\"] = \"no_match\"\n                row[\"matched_party\"] = \"no_match\"\n                row[\"matched_office\"] = \"no_match\"\n                row[\"matched_year\"] = \"no_match\"\n                row[\"matched_method\"] = \"no_match\"\n                row[\"matched_score\"] = \"no_match\"\n                row[\"matched_state\"] = \"no_match\"\n                row[\"without_state_and_match\"] = False\n                return row\n\n            try:\n                first_name = row[\"committee_member\"].replace(last_name, \"\").split()[0]\n            except Exception:\n                try:\n                    first_name = row[\"committee_member\"].split()[0]\n                except Exception:\n                    first_name = \"\"\n\n            # Ensure 'last_name' is a list\n            possible_last_names = congress_possible[\"last_name\"].tolist()\n            last_name_results = process.extract(last_name, possible_last_names)\n            # Only keep results with score &gt; 80\n            last_name_results = [\n                result for result in last_name_results if result[1] &gt; 80\n            ]\n            congress_match_lastname = congress_possible[\n                congress_possible[\"last_name\"].isin(\n                    [result[0] for result in last_name_results]\n                )\n            ]\n            congress_match_lastname[\"first_name\"] = \"\"\n            for j, row_lastname in congress_match_lastname.iterrows():\n                try:\n                    congress_match_lastname.at[j, \"first_name\"] = (\n                        row_lastname[\"candidate\"]\n                        .str.replace(row_lastname[\"last_name\"], \"\")\n                        .split()[0]\n                    )\n                except Exception:\n                    try:\n                        congress_match_lastname.at[j, \"first_name\"] = row_lastname[\n                            \"candidate\"\n                        ].split()[0]\n                    except Exception:\n                        continue\n\n            congress_match_firstnamelist = congress_match_lastname[\n                \"first_name\"\n            ].tolist()\n            first_name_results = (\n                process.extract(first_name, congress_match_firstnamelist)\n                if first_name != \"\"\n                else None\n            )\n            if first_name_results is not None:\n                first_name_results = [\n                    result for result in first_name_results if result[1] &gt; 40\n                ]\n                congress_match_lastandfirst = congress_match_lastname[\n                    congress_match_lastname[\"first_name\"].isin(\n                        [result[0] for result in first_name_results]\n                    )\n                ]\n\n            if (\n                first_name_results is None\n                or congress_match_firstnamelist is None\n                or len(congress_match_firstnamelist) == 0\n            ):\n                # STEP 3: ONLY MATCH LAST NAME, MOST INACCRURATE\n                if len(congress_match_lastname) &gt;= 1:\n                    if pd.isna(state):\n                        row[\"matched_name\"] = congress_match_lastname[\n                            \"candidate\"\n                        ].values[0]\n                        row[\"matched_party\"] = congress_match_lastname[\"party\"].values[\n                            0\n                        ]\n                        row[\"matched_office\"] = congress_match_lastname[\n                            \"office\"\n                        ].values[0]\n                        row[\"matched_year\"] = congress_match_lastname[\"year\"].values[0]\n                        row[\"matched_method\"] = \"last_name_only\"\n                        row[\"matched_score\"] = last_name_results[0][1]\n                        row[\"matched_state\"] = congress_match_lastname[\n                            \"state_po_original\"\n                        ].values[0]\n                        row[\"without_state_and_match\"] = True\n                    else:\n                        congress_match_lastname = congress_match_lastname[\n                            congress_match_lastname[\"state_po\"] == state\n                        ]\n                        if len(congress_match_lastname) &gt;= 1:\n                            row[\"matched_name\"] = congress_match_lastname[\n                                \"candidate\"\n                            ].values[0]\n                            row[\"matched_party\"] = congress_match_lastname[\n                                \"party\"\n                            ].values[0]\n                            row[\"matched_office\"] = congress_match_lastname[\n                                \"office\"\n                            ].values[0]\n                            row[\"matched_year\"] = congress_match_lastname[\n                                \"year\"\n                            ].values[0]\n                            row[\"matched_method\"] = \"last_name_only\"\n                            row[\"matched_score\"] = last_name_results[0][1]\n                            row[\"matched_state\"] = congress_match_lastname[\n                                \"state_po_original\"\n                            ].values[0]\n                            row[\"without_state_and_match\"] = False\n                        else:\n                            print(f\"No match: {name} in {year}\")\n                            row[\"matched_name\"] = \"no_match\"\n                            row[\"matched_party\"] = \"no_match\"\n                            row[\"matched_office\"] = \"no_match\"\n                            row[\"matched_year\"] = \"no_match\"\n                            row[\"matched_method\"] = \"no_match\"\n                            row[\"matched_score\"] = \"no_match\"\n                            row[\"matched_state\"] = \"no_match\"\n                            row[\"without_state_and_match\"] = False\n                else:\n                    row[\"matched_name\"] = \"no_match\"\n                    row[\"matched_party\"] = \"no_match\"\n                    row[\"matched_office\"] = \"no_match\"\n                    row[\"matched_year\"] = \"no_match\"\n                    row[\"matched_method\"] = \"no_match\"\n                    row[\"matched_score\"] = \"no_match\"\n                    row[\"matched_state\"] = \"no_match\"\n                    row[\"without_state_and_match\"] = False\n\n            else:\n                # STEP 2: MATCH LAST NAME AND THEN FIRST NAME, FIRST NAME SCORE &gt; 40\n                if len(congress_match_lastandfirst) &gt;= 1:\n                    if pd.isna(state):\n                        row[\"matched_name\"] = congress_match_lastandfirst[\n                            \"candidate\"\n                        ].values[0]\n                        row[\"matched_party\"] = congress_match_lastandfirst[\n                            \"party\"\n                        ].values[0]\n                        row[\"matched_office\"] = congress_match_lastandfirst[\n                            \"office\"\n                        ].values[0]\n                        row[\"matched_year\"] = congress_match_lastandfirst[\n                            \"year\"\n                        ].values[0]\n                        row[\"matched_method\"] = \"last_and_first\"\n                        row[\"matched_score\"] = last_name_results[0][1]\n                        row[\"matched_state\"] = congress_match_lastandfirst[\n                            \"state_po_original\"\n                        ].values[0]\n                        row[\"without_state_and_match\"] = True\n                    else:\n                        congress_match_lastandfirst = congress_match_lastandfirst[\n                            congress_match_lastandfirst[\"state_po\"] == state\n                        ]\n                        if len(congress_match_lastandfirst) &gt;= 1:\n                            row[\"matched_name\"] = congress_match_lastandfirst[\n                                \"candidate\"\n                            ].values[0]\n                            row[\"matched_party\"] = congress_match_lastandfirst[\n                                \"party\"\n                            ].values[0]\n                            row[\"matched_office\"] = congress_match_lastandfirst[\n                                \"office\"\n                            ].values[0]\n                            row[\"matched_year\"] = congress_match_lastandfirst[\n                                \"year\"\n                            ].values[0]\n                            row[\"matched_method\"] = \"last_and_first\"\n                            row[\"matched_score\"] = last_name_results[0][1]\n                            row[\"matched_state\"] = congress_match_lastandfirst[\n                                \"state_po_original\"\n                            ].values[0]\n                            row[\"without_state_and_match\"] = False\n                        else:\n                            print(f\"No match: {name} in {year}\")\n                            row[\"matched_name\"] = \"no_match\"\n                            row[\"matched_party\"] = \"no_match\"\n                            row[\"matched_office\"] = \"no_match\"\n                            row[\"matched_year\"] = \"no_match\"\n                            row[\"matched_method\"] = \"no_match\"\n                            row[\"matched_score\"] = \"no_match\"\n                            row[\"matched_state\"] = \"no_match\"\n                            row[\"without_state_and_match\"] = False\n\n        return row\n\n\ndef preprocess_dataframe(df, file_name):\n    \"\"\"\n    This function serves to preprocess the dataframe for matching with the committee members.\n    \"\"\"\n\n    df = df[df[\"file\"] == file_name].copy()\n    df[\"speaker_original\"] = df[\"speaker\"]\n    df[\"section\"] = df[\"section\"].str.lower()\n\n    prefix_list = [\n        \"mr\",\n        \"mrs\",\n        \"ms\",\n        \"miss\",\n        \"dr\",\n        \"prof\",\n        \"representative\",\n        \"senator\",\n        \"chairman\",\n        \"chairwoman\",\n        \"vice\",\n        \"chair\",\n    ]\n\n    # Create a regex pattern for prefixes, including case insensitivity and optional period\n    prefix_pattern = r\"(?i)^\\b(\" + \"|\".join(prefix_list) + r\")[\\.\\s]*\"\n\n    # Replace prefixes in a case-insensitive manner\n    df[\"speaker\"] = df[\"speaker\"].str.replace(prefix_pattern, \"\", regex=True)\n    df[\"speaker\"] = df[\"speaker\"].str.lower().str.replace(r\"\\.\", \"\", regex=True)\n\n    return df\n\n\ndef ensure_columns_exist(df, columns):\n    \"\"\"\n    This function serves to ensure that the columns exist in the dataframe.\n    \"\"\"\n\n    for column in columns:\n        if column not in df.columns:\n            df[column] = None\n\n\ndef extract_chair_lastnames(speakers):\n    \"\"\"\n    This function serves to extract the chair lastnames from the speakers list.\n    \"\"\"\n    chair_list = [s for s in speakers if \"chair\" in s]\n    chair_list = [s for s in chair_list if \"the\" not in s]\n    chair_lastname_list = []\n    if chair_list:\n        for entry in chair_list:\n            words = entry.split()\n            for i, word in enumerate(words):\n                if \"chair\" in word and i + 1 &lt; len(words):\n                    chair_lastname_list.append(words[i + 1])\n    else:\n        chair_lastname_list = []\n\n    return chair_list, chair_lastname_list\n\n\ndef match_chair(dfh, dfc, chair_list, chair_lastname_list):\n    \"\"\"\n    This function serves to match the chair with the committee members.\n    \"\"\"\n\n    if not chair_lastname_list:\n        dfc_temp = dfc[dfc[\"identity\"].str.contains(\"chair\", case=False)]\n        dfc_temp = dfc_temp[~dfc_temp[\"identity\"].str.contains(\"vice\", case=False)]\n        if len(dfc_temp) &gt; 1:\n            indices = dfh[\n                (dfh[\"speaker\"].str.contains(\"chair\", case=False))\n                &amp; (~dfh[\"speaker\"].str.contains(\"vice\", case=False))\n            ].index\n            dfh.loc[indices, \"matched\"] = len(dfc_temp)\n            dfh.loc[indices, \"matched_name\"] = dfc_temp[\"matched_name\"].str.cat(\n                sep=\" &amp; \"\n            )\n            dfh.loc[\n                indices,\n                [\n                    \"matched_year\",\n                    \"matched_state\",\n                    \"matched_identity\",\n                    \"matched_office\",\n                    \"matched_method\",\n                    \"matched_committee\",\n                ],\n            ] = dfc_temp[\n                [\n                    \"matched_year\",\n                    \"matched_state\",\n                    \"matched_identity\",\n                    \"matched_office\",\n                    \"matched_method\",\n                    \"matched_committee\",\n                ]\n            ].apply(lambda x: x.str.cat(sep=\" &amp; \"))\n        elif len(dfc_temp) == 1:\n            indices = dfh[(dfh[\"speaker\"].str.contains(\"chair\", case=False))].index\n            dfh.loc[indices, \"matched\"] = 1\n            for col in [\n                \"matched_name\",\n                \"matched_year\",\n                \"matched_state\",\n                \"matched_identity\",\n                \"matched_office\",\n                \"matched_method\",\n                \"matched_committee\",\n            ]:\n                dfh.loc[indices, col] = dfc_temp[col].values[0]\n\n    else:\n        match_lastnames(dfh, dfc, chair_lastname_list, \"chair\")\n\n\ndef extract_vice_chair_lastnames(speakers):\n    \"\"\"\n    This function serves to extract the vice chair lastnames from the speakers list.\n    \"\"\"\n\n    vice_chair_list = [s for s in speakers if \"vice\" in s]\n    vice_chair_list = [s for s in vice_chair_list if \"the\" not in s]\n    vice_chair_lastname_list = []\n    for entry in vice_chair_list:\n        words = entry.split()\n        for i, word in enumerate(words):\n            if \"vice\" in word and i + 1 &lt; len(words):\n                vice_chair_lastname_list.append(words[i + 1])\n    return vice_chair_list, vice_chair_lastname_list\n\n\ndef match_vice_chair(dfh, dfc, vice_chair_list, vice_chair_lastname_list):\n    \"\"\"\n    This function serves to match the vice chair with the committee members.\n    \"\"\"\n\n    if not vice_chair_lastname_list:\n        dfc_temp = dfc[dfc[\"identity\"].str.contains(\"vice\", case=False)]\n        update_dataframe(dfh, dfc_temp, vice_chair_list, \"vice-chair\")\n    else:\n        match_lastnames(dfh, dfc, vice_chair_lastname_list, \"vice-chair\")\n\n\ndef update_dataframe(dfh, dfc_temp, speaker_list, role):\n    if len(dfc_temp) &gt; 1:\n        update_multiple_matches(dfh, dfc_temp, speaker_list)\n    elif len(dfc_temp) == 1:\n        update_single_match(dfh, dfc_temp, speaker_list)\n    else:\n        update_no_match(dfh, speaker_list)\n\n\ndef update_multiple_matches(dfh, dfc_temp, speaker_list):\n    indices = dfh[dfh[\"speaker\"].isin(speaker_list)].index\n    dfh.loc[indices, \"matched\"] = len(dfc_temp)\n    for col in [\n        \"matched_name\",\n        \"matched_year\",\n        \"matched_state\",\n        \"matched_identity\",\n        \"matched_office\",\n        \"matched_method\",\n        \"matched_committee\",\n    ]:\n        dfh.loc[indices, col] = dfc_temp[col].str.cat(sep=\" &amp; \")\n\n\ndef update_single_match(dfh, dfc_temp, speaker_list):\n    indices = dfh[dfh[\"speaker\"].isin(speaker_list)].index\n    dfh.loc[indices, \"matched\"] = 1\n    for col in [\n        \"matched_name\",\n        \"matched_year\",\n        \"matched_state\",\n        \"matched_identity\",\n        \"matched_office\",\n        \"matched_method\",\n        \"matched_committee\",\n    ]:\n        dfh.loc[indices, col] = dfc_temp[col].values[0]\n\n\ndef update_no_match(dfh, speaker_list):\n    indices = dfh[dfh[\"speaker\"].isin(speaker_list)].index\n    dfh.loc[indices, \"matched\"] = 0\n    dfh.loc[\n        indices,\n        [\n            \"matched_name\",\n            \"matched_year\",\n            \"matched_state\",\n            \"matched_office\",\n            \"matched_method\",\n            \"matched_committee\",\n        ],\n    ] = \"no_match\"\n    dfh.loc[indices, \"matched_identity\"] = \"no_identity\"\n\n\ndef match_lastnames(dfh, dfc, lastname_list, role):\n    \"\"\"\n    This function serves to match the lastnames with the committee members.\n    \"\"\"\n\n    dfc[\"last_name\"] = dfc[\"last_name\"].fillna(\"\")\n    dfc[\"matched_name\"] = dfc[\"matched_name\"].fillna(\"\")\n    for entry in lastname_list:\n        result = process.extractOne(entry, dfc[\"matched_name\"])\n        if result and result[1] &gt; 80:\n            matched_row = dfc[dfc[\"matched_name\"] == result[0]]\n            update_single_match(dfh, matched_row, [entry])\n        else:\n            update_no_match(dfh, [entry])\n\n\ndef match_remaining_speakers(dfh, dfc, speakers, prefix_list):\n    dfc[\"last_name\"] = dfc[\"last_name\"].fillna(\"\")\n    dfc[\"matched_name\"] = dfc[\"matched_name\"].fillna(\"\")\n    speaker_lastname_list = [\n        s.replace(prefix, \"\").strip() for s in speakers for prefix in prefix_list\n    ]\n    potential_speakers = dfc[\"matched_name\"].tolist()\n    potential_speakers_lastname = [\n        s.split()[-1] for s in potential_speakers if s\n    ]  # Ensure non-empty strings\n\n    for last_name in speaker_lastname_list:\n        result = process.extractOne(last_name, potential_speakers_lastname)\n        if result and result[1] &gt; 80:\n            matched_row = dfc[dfc[\"matched_name\"].str.contains(result[0], na=False)]\n            if len(matched_row) == 1:\n                update_single_match(dfh, matched_row, [last_name])\n            elif len(matched_row) &gt; 1:\n                update_multiple_matches(dfh, matched_row, [last_name])\n            else:\n                update_no_match(dfh, [last_name])\n        else:\n            update_no_match(dfh, [last_name])\n\n\ndef match_hearing_with_committee_member(tomatch, matched, file_name):\n    \"\"\"\n    This is the main function to match the hearing with the committee members.\n    \"\"\"\n\n    dfh = preprocess_dataframe(tomatch, file_name)\n    dfc = matched[matched[\"file_name\"] == file_name].copy()\n\n    ensure_columns_exist(\n        dfh,\n        [\n            \"matched\",\n            \"matched_name\",\n            \"matched_year\",\n            \"matched_state\",\n            \"matched_identity\",\n            \"matched_office\",\n            \"matched_method\",\n            \"matched_committee\",\n        ],\n    )\n\n    speakers = dfh[\"speaker\"].unique().tolist()\n\n    chair_list, chair_lastname_list = extract_chair_lastnames(speakers)\n    match_chair(dfh, dfc, chair_list, chair_lastname_list)\n\n    vice_chair_list, vice_chair_lastname_list = extract_vice_chair_lastnames(speakers)\n    match_vice_chair(dfh, dfc, vice_chair_list, vice_chair_lastname_list)\n\n    remaining_speakers = [\n        s for s in speakers if s not in chair_list and s not in vice_chair_list\n    ]\n    remaining_speakers = [\n        s for s in remaining_speakers if \"chair\" not in s and \"vice\" not in s\n    ]\n    prefix_list = [\"mr\", \"mrs\", \"ms\", \"miss\", \"dr\", \"prof\", \"representative\", \"senator\"]\n    match_remaining_speakers(dfh, dfc, remaining_speakers, prefix_list)\n\n    return dfh\n\n\ndef hearing_sentiment(df, pipe_model=pipe, batch_size=10000):\n    \"\"\"\n    This function serves to estimate sentiment to the hearing text.\n    \"\"\"\n\n    sentiments = []\n    sentiment_scores = []\n\n    for start in range(0, len(df), batch_size):\n        end = min(start + batch_size, len(df))\n        text_list = df[start:end][\"text\"].tolist()\n\n        results = pipe_model(text_list)\n\n        sentiments.extend([result[\"label\"] for result in results])\n        sentiment_scores.extend([result[\"score\"] for result in results])\n\n    # Assign the results back to the dataframe\n    df[\"sentiment\"] = sentiments\n    df[\"sentiment_score\"] = sentiment_scores\n\n    return df\n\n\ndef find_possible_ocr_typo(text, target, threshold=90):\n    n = len(text)\n    matches = []\n\n    for i in range(n):\n        for j in range(i + 1, n + 1):\n            substring = text[i:j]\n            score = fuzz.ratio(target, substring)\n            if score &gt;= threshold:\n                matches.append((substring, score))\n\n    matches = sorted(set(matches), key=lambda x: x[1], reverse=True)\n\n    def is_mostly_uppercase(s):\n        uppercase_count = sum(1 for c in s if c.isupper())\n        return uppercase_count &gt; len(s) / 2\n\n    filtered_matches = [m[0] for m in matches if is_mostly_uppercase(m[0])]\n\n    return filtered_matches\n\n\ndef update_ocr_df(df, file, threshold=90):\n    df_temp = df[df[\"file\"] == file].copy()\n    target_list = df_temp[\"speaker\"].unique().tolist()\n\n    for i, row in df_temp.iterrows():\n        output = {}\n        text = row[\"text\"]\n        for target in target_list:\n            matches = find_possible_ocr_typo(text, target, threshold)\n            if matches:\n                output[target] = matches[0]\n        if output:\n            df_temp.at[i, \"ocr_corrections\"] = output\n\n    return df_temp\n\n\ndef parse_text(text_original):\n    results = {}\n    # Step 1: Deal with Prefix Errors\n    text = text_original.lower().replace(\"  \", \" \").replace(\"\\n\", \" \")\n\n    speaker_regex = re.compile(\n        r\"(mr\\.|mrs\\.|ms\\.|miss|senator|representative|chairman|vice|dr\\.)\\s([^\\s]+)\"\n    )\n    # how to be more accurate in prefixes?\n    speaker_matches = re.findall(speaker_regex, text)\n    # print(speaker_matches)\n\n    for prefix, candidate in speaker_matches:\n        full_candidate = f\"{prefix} {candidate}\"\n        # only consider about capital situation of candidate not prefix regarding \"Senator XXXX\" situation\n        for match in re.finditer(re.escape(candidate), text_original, re.IGNORECASE):\n            original_candidate = match.group(0)\n            upper_count = sum(1 for c in original_candidate if c.isupper())\n            if upper_count &gt; len(original_candidate) * 0.5 and upper_count &gt; 3:\n                original_idx = match.start()\n                results[full_candidate] = original_idx\n\n    # Step 2: Extract Capitalized Words and compare with \"CHAIRMAN\"\n    words = text_original.split()\n    valid_words = []\n    for word in words:\n        if len(word) &gt; 3:\n            upper_count = sum(1 for c in word if c.isupper())\n            if upper_count &gt; len(word) * 0.7:\n                valid_words.append(word)\n\n    for word in valid_words:\n        if \"VICE\" in word:\n            results[f\"vice-chair_{word}\"] = text_original.find(word)\n            valid_words.remove(word)\n            break\n\n        if \"CHAIR\" in word:\n            results[f\"chair_{word}\"] = text_original.find(word)\n            valid_words.remove(word)\n            break\n\n    # Irregular words\n    for word in valid_words:\n        scores_chair = [\n            fuzz.ratio(word, \"CHAIRMAN\"),\n            fuzz.ratio(word, \"CHAIRWOMAN\"),\n            fuzz.ratio(word, \"CHAIR\"),\n        ]\n\n        if any(score &gt; 70 for score in scores_chair):\n            original_idx = text_original.find(word)\n            results[\"chair\"] = original_idx\n\n        scores_vicechair = [fuzz.ratio(word, \"VICECHAIR\"), fuzz.ratio(word, \"VICE\")]\n\n        if any(score &gt; 70 for score in scores_vicechair):\n            original_idx = text_original.find(word)\n            results[\"vice-chair\"] = original_idx\n\n    results = dict(sorted(results.items(), key=lambda item: item[1]))\n\n    # Step 3: Remove Duplicates (to specify, when two keys share a same value, remove the shorter one)\n    to_remove = []\n\n    for key, value in results.items():\n        for k, v in results.items():\n            if k != key and v == value:\n                if len(k) &gt; len(key):\n                    to_remove.append(key)\n                else:\n                    to_remove.append(k)\n\n    for key in set(to_remove):\n        results.pop(key, None)\n\n    return results\n\n\ndef line_change(row, text_original, results):\n    if not results:\n        row = row.copy()\n        row[\"note\"] = \"no_error\"\n        return pd.DataFrame(row).T\n\n    new_rows = []\n    text_split = {}\n    length = len(text_original)\n\n    split_points = sorted(results.values())\n\n    if split_points[0] != 0:\n        split_points.insert(0, 0)\n\n    if split_points[-1] != length:\n        split_points.append(length)\n\n    for i in range(len(split_points) - 1):\n        start_idx = split_points[i]\n        end_idx = split_points[i + 1]\n        segment = text_original[start_idx:end_idx].strip()\n\n        if start_idx == 0:\n            speaker = row[\"speaker\"]\n        else:\n            speaker = next(key for key, value in results.items() if value == start_idx)\n\n        text_split[start_idx] = {\"text\": segment, \"speaker\": speaker}\n\n    for idx, segment in text_split.items():\n        if idx == 0:\n            row_original = row.copy()\n            row_original[\"text\"] = segment[\"text\"]\n            row_original[\"note\"] = \"first_part\"\n            row_original[\"speaker\"] = segment[\"speaker\"]\n            row_original[\"line_changes\"] = results\n            new_rows.append(row_original)\n\n        else:\n            row_new = row.copy()\n            row_new[\"text\"] = segment[\"text\"]\n            row_new[\"note\"] = \"latter_part\"\n            row_new[\"speaker\"] = segment[\"speaker\"]\n            row_new[\"line_changes\"] = results\n            new_rows.append(row_new)\n\n    new_df = None\n\n    new_rows = [new_row for new_row in new_rows if new_row.notna().any()]\n    for new_row in new_rows:\n        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n\n    return new_df.reset_index(drop=True)\n\n\ndef fix_line_change(idx, df):\n    row = df.loc[idx]\n    text_original = row[\"text\"]\n    results = parse_text(text_original)\n    return line_change(row, text_original, results)\n\n\n# chairman_regex = f\"({\"|\".join(chairman_ocr_errors})\"\n# re.sub(pattern, 'THE CHAIRMAN.', string)\n\n\ndef detect_mostly_capital_sentence(sentence):\n    \"\"\"\n    Is mostly capital: over 60% of the characters are mostly capital and after removing the non-mostly capital words,\n    # the sentence is still longer than 5 characters.\n    \"\"\"\n    words = sentence.split()\n    length = len(words)\n\n    if length &lt;= 5:\n        return False\n\n    else:\n        upper_word_count = 0\n        for word in words:\n            capital_count = sum(1 for c in word if c.isupper())\n            if capital_count &gt; len(word) * 0.6:\n                upper_word_count += 1\n\n        if upper_word_count &lt;= length * 0.6:\n            return False\n\n        else:\n            return True\n\n\ndef split_text_into_sentences_spacy(text):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n    sentences = [sent.text.strip() for sent in doc.sents]\n\n    return sentences\n\n\ndef split_text_into_sentences_re(text):\n    sentence_endings = re.compile(r\"(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?|!)\\s\")\n    sentences = sentence_endings.split(text)\n    # cleaned_sentences = [re.sub(r'[^a-zA-Z\\s]', '', sentence).strip() for sentence in sentences if sentence]\n    return sentences\n\n\ndef detect_section_heading(text):\n    results = []\n    sentences = split_text_into_sentences_re(text)\n\n    for sentence in sentences:\n        if detect_mostly_capital_sentence(sentence):\n            results.append(sentence)\n\n    return results\n\n\ndef is_sentence(text, nlp):\n    \"\"\"\n    This function serves to determine whether the text is a sentence.\n    \"\"\"\n\n    doc = nlp(text)\n\n    contains_verb = any(token.pos_ == \"VERB\" for token in doc)\n    contains_subject = any(token.dep_ == \"nsubj\" for token in doc)\n    contains_object = any(token.dep_ == \"dobj\" for token in doc)\n    contains_punctuation = any(token.is_punct for token in doc)\n    contains_proper_noun = any(token.pos_ == \"PROPN\" for token in doc)\n\n    score = 0\n    if contains_verb:\n        score += 1\n    if contains_subject:\n        score += 1\n    if contains_object:\n        score += 1\n    if contains_punctuation:\n        score += 1\n    if contains_proper_noun:\n        score -= 1\n\n    return score\n\n\ndef deal_with_multiple_matched(row):\n    \"\"\"\n    This function serves to deal with multiple matches.\n    \"\"\"\n\n    row = row.copy()\n    number = int(row[\"matched\"])\n    if number == 0:\n        return row\n\n    matched_name = row[\"matched_name\"]\n    matched_year = row[\"matched_year\"]\n    matched_state = row[\"matched_state\"]\n    matched_identity = row[\"matched_identity\"]\n    matched_office = row[\"matched_office\"]\n    matched_method = row[\"matched_method\"]\n    matched_committee = row[\"matched_committee\"]\n\n    matched_name = matched_name if not pd.isna(matched_name) else \"\"\n    matched_year = matched_year if not pd.isna(matched_year) else \"\"\n    matched_state = matched_state if not pd.isna(matched_state) else \"\"\n    matched_identity = matched_identity if not pd.isna(matched_identity) else \"\"\n    matched_office = matched_office if not pd.isna(matched_office) else \"\"\n    matched_method = matched_method if not pd.isna(matched_method) else \"\"\n    matched_committee = matched_committee if not pd.isna(matched_committee) else \"\"\n\n    if number == 1:\n        if \"no_match\" in matched_name:\n            row[\"matched\"] = 0\n            return row\n        else:\n            return row\n\n    if matched_name == \"\" or matched_year == \"\" or matched_name == \"no_match\":\n        row[\"matched\"] = 0\n        row[\"matched_name\"] = \"no_match\"\n        row[\"matched_year\"] = \"no_match\"\n        row[\"matched_state\"] = \"no_match\"\n        row[\"matched_identity\"] = \"no_identity\"\n        row[\"matched_office\"] = \"no_match\"\n        row[\"matched_method\"] = \"no_match\"\n        row[\"matched_committee\"] = \"no_match\"\n        return row\n\n    # remaining situation: number &gt;= 2\n    if pd.isna(matched_state):\n        matched_state = \"notapplicable &amp; \" * number\n        matched_state = re.sub(r\" &amp; $\", \"\", matched_state)\n\n    names = matched_name.split(\" &amp; \")\n    years = matched_year.split(\" &amp; \")\n    states = matched_state.split(\" &amp; \")\n    identities = matched_identity.split(\" &amp; \")\n    offices = matched_office.split(\" &amp; \")\n    methods = matched_method.split(\" &amp; \")\n    committees = matched_committee.split(\" &amp; \")\n\n    if len(states) != number:\n        matched_state = \"notapplicable &amp; \" * number\n        matched_state = re.sub(r\" &amp; $\", \"\", matched_state)\n        states = matched_state.split(\" &amp; \")\n\n    # connections can only sort out no_match\n    connections = []\n    for i in range(number):\n        connection = f\"{names[i]} | {years[i]} | {states[i]} | {identities[i]} | {offices[i]} | {methods[i]} | {committees[i]}\"\n        connections.append(connection)\n\n    connections = list(set(connections))\n\n    connections = [x for x in connections if \"no_match\" not in x]\n\n    length = len(connections)\n\n    row[\"matched\"] = length\n\n    if length == 0:\n        row[\"matched_name\"] = \"no_match\"\n        row[\"matched_year\"] = \"no_match\"\n        row[\"matched_state\"] = \"no_match\"\n        row[\"matched_identity\"] = \"no_identity\"\n        row[\"matched_office\"] = \"no_match\"\n        row[\"matched_method\"] = \"no_match\"\n        row[\"matched_committee\"] = \"no_match\"\n        return row\n\n    if length == 1:  # multiple matches filtering \"no_match\" to only one\n        single_connection = connections[0].split(\" | \")\n        row[\"matched_name\"] = single_connection[0]\n        row[\"matched_year\"] = single_connection[1]\n        row[\"matched_state\"] = single_connection[2]\n        row[\"matched_identity\"] = single_connection[3]\n        row[\"matched_office\"] = single_connection[4]\n        row[\"matched_method\"] = single_connection[5]\n        row[\"matched_committee\"] = single_connection[6]\n        return row\n\n    if length &gt;= 1:\n        # deal with same person, different year or committee situation, depends only on name, state, office\n        names = list(set(names))\n        states = list(set(states))\n        offices = list(set(offices))\n\n        if len(names) == 1 and len(states) == 1 and len(offices) == 1:\n            row[\"matched\"] = 1\n            row[\"matched_name\"] = names[0]\n            row[\"matched_year\"] = years[0]\n            row[\"matched_state\"] = states[0]\n            row[\"matched_identity\"] = identities[0]\n            row[\"matched_office\"] = offices[0]\n            row[\"matched_method\"] = methods[0]\n            row[\"matched_committee\"] = committees[0]\n            return row\n\n        # deal with same person but recorded with nicknames situation\n        if len(states) == 1 and len(offices) == 1:\n            last_names = [name.split()[-1] for name in names]\n            last_names = list(set(last_names))\n            if len(last_names) == 1:\n                row[\"matched\"] = 1\n                row[\"matched_name\"] = names[0]\n                row[\"matched_year\"] = years[0]\n                row[\"matched_state\"] = states[0]\n                row[\"matched_identity\"] = identities[0]\n                row[\"matched_office\"] = offices[0]\n                row[\"matched_method\"] = methods[0]\n                row[\"matched_committee\"] = committees[0]\n                return row\n\n        # remaining situations: split up connections again to reorganize the row\n        names = []\n        years = []\n        states = []\n        identities = []\n        offices = []\n        methods = []\n        committees = []\n\n        for connection in connections:\n            name, year, state, identity, office, method, committee = connection.split(\n                \" | \"\n            )\n            names.append(name)\n            years.append(year)\n            states.append(state)\n            identities.append(identity)\n            offices.append(office)\n            methods.append(method)\n            committees.append(committee)\n\n        row[\"matched_name\"] = \" &amp; \".join(names)\n        row[\"matched_year\"] = \" &amp; \".join(years)\n        row[\"matched_state\"] = \" &amp; \".join(states)\n        row[\"matched_identity\"] = \" &amp; \".join(identities)\n        row[\"matched_office\"] = \" &amp; \".join(offices)\n        row[\"matched_method\"] = \" &amp; \".join(methods)\n        row[\"matched_committee\"] = \" &amp; \".join(committees)\n\n        return row\n\n    return row\n\n\ndef delete_undeleted_names(df):\n    \"\"\"\n    This function serves to delete the undeleted names.\n    \"\"\"\n    speaker_regex = re.compile(\n        r\"(mr\\.|mrs\\.|ms\\.|miss|senator|representative|chairman|vice|dr\\.)\\s([^\\s]+)\",\n        re.IGNORECASE,\n    )\n\n    for i in range(1, len(df)):\n        name = df.at[i, \"speaker\"].lower()\n        if \"chair\" in name:\n            previous_text = df.at[i - 1, \"text\"].rstrip()\n            current_text = df.at[i, \"text\"]\n            current_text = re.sub(r\"^\\s*\", \"\", current_text).lstrip()\n\n            # Extract last word from previous_text and first word from current_text\n            previous_last_word = previous_text.split()[-1] if previous_text else \"\"\n            current_first_word = current_text.split()[0] if current_text else \"\"\n\n            # Calculate fuzzy matching scores\n            previous_score = fuzz.ratio(previous_last_word, \"THE\")\n            first_scores = [\n                fuzz.ratio(current_first_word, \"CHAIRMAN\"),\n                fuzz.ratio(current_first_word, \"CHAIRWOMAN\"),\n                fuzz.ratio(current_first_word, \"CHAIR\"),\n            ]\n\n            if previous_score &gt; 70:\n                previous_text = previous_text[: -len(previous_last_word)].rstrip()\n\n            if any(score &gt; 70 for score in first_scores):\n                current_text = re.sub(\n                    re.escape(current_first_word), \"\", current_text, flags=re.IGNORECASE\n                ).lstrip()\n\n            df.at[i - 1, \"text\"] = previous_text\n            df.at[i, \"text\"] = current_text\n\n        # Check if the 'note' column is 'latter_part'\n        if df.at[i, \"note\"] == \"latter_part\":\n            match = speaker_regex.match(name)\n\n            if match:\n                prefix = match.group(1)\n                candidate = match.group(2)\n\n                # Remove prefix from the end of the previous row's 'text' column\n                previous_text = df.at[i - 1, \"text\"].rstrip()\n                if previous_text.lower().endswith(prefix.lower()):\n                    df.at[i - 1, \"text\"] = previous_text[: -len(prefix)].rstrip()\n\n                # Remove candidate from the start of the current row's 'text' column\n                current_text = df.at[i, \"text\"]\n                candidate_pattern = re.compile(\n                    r\"^\\s*\" + re.escape(candidate), re.IGNORECASE\n                )\n                current_text = re.sub(candidate_pattern, \"\", current_text).lstrip()\n                df.at[i, \"text\"] = current_text\n\n    return df\n\n\ndef spell_check(text, spell):\n    \"\"\"\n    This function serves to spell check the text.\n    \"\"\"\n    if not isinstance(text, str):\n        return 0, 0, 0\n\n    text = re.sub(r\"[^a-zA-Z\\s\\']\", \"\", text).lower()\n    words = text.split()\n    unknown_words = spell.unknown(words)\n    total_count = len(words)\n\n    short_word_count = 0\n\n    for word in words:\n        if len(word) &lt;= 1 and word != \"i\":\n            short_word_count += 1\n\n    return len(unknown_words), total_count, short_word_count\n</code></pre>"},{"location":"tfidf-sample/","title":"Sample Code 2","text":""},{"location":"tfidf-sample/#sample-code-2","title":"Sample Code 2","text":"<p>This Python code demonstrates text similarity matching using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity. The code implements efficient blocking and matching functions for comparing large sets of company names or similar text data.</p> <ul> <li>Text Processing: N-gram generation and tokenization</li> <li>Similarity Calculation: Optimized cosine similarity using sparse matrices</li> <li>TF-IDF Vectorization: Document feature extraction</li> <li>Efficient Blocking: Strategic data partitioning for large datasets</li> </ul>"},{"location":"tfidf-sample/#code","title":"Code","text":"<pre><code>import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport sparse_dot_topn.sparse_dot_topn as ct   \n\nfrom os.path import join\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer  \nfrom scipy.sparse import csr_matrix\n\n\n# Text Processing Functions\ndef ngrams(string, n=3):\n    \"\"\"Generate n-grams from string after removing specific characters.\"\"\"\n    string = re.sub(r'[,-./]|\\sBD',r'', string)\n    ngrams = zip(*[string[i:] for i in range(n)])\n    return [''.join(ngram) for ngram in ngrams]\n\n# Similarity Calculation Functions\ndef cos_sim_top(matrix_a, matrix_b, n_top_matches, similarity_threshold=0):\n    \"\"\"\n    Calculate cosine similarity between two sparse matrices efficiently and return top matches.\n\n    Args:\n        matrix_a (sparse matrix): First input sparse matrix\n        matrix_b (sparse matrix): Second input sparse matrix\n        n_top_matches (int): Number of top matches to return\n        similarity_threshold (float): Minimum similarity threshold (default: 0)\n\n    Returns:\n        scipy.sparse.csr_matrix: Sparse matrix containing top similarity scores\n    \"\"\"\n    # Convert input matrices to CSR format for efficient computation\n    matrix_a = matrix_a.tocsr()\n    matrix_b = matrix_b.tocsr()\n\n    # Get matrix dimensions\n    rows_a, _ = matrix_a.shape\n    _, cols_b = matrix_b.shape\n\n    # Initialize arrays for result storage\n    index_dtype = np.int32\n    max_nonzero = rows_a * n_top_matches\n\n    result_indptr = np.zeros(rows_a + 1, dtype=index_dtype)\n    result_indices = np.zeros(max_nonzero, dtype=index_dtype)\n    result_data = np.zeros(max_nonzero, dtype=matrix_a.dtype)\n\n    # Perform sparse dot product using compiled function\n    ct.sparse_dot_topn(\n        rows_a, cols_b,\n        np.asarray(matrix_a.indptr, dtype=index_dtype),\n        np.asarray(matrix_a.indices, dtype=index_dtype),\n        matrix_a.data,\n        np.asarray(matrix_b.indptr, dtype=index_dtype),\n        np.asarray(matrix_b.indices, dtype=index_dtype),\n        matrix_b.data,\n        n_top_matches,\n        similarity_threshold,\n        result_indptr, result_indices, result_data\n    )\n\n    # Return result as CSR matrix\n    return csr_matrix((result_data, result_indices, result_indptr), \n                     shape=(rows_a, cols_b))\n\ndef get_matches_dataframe(similarity_matrix, names_a, names_b, limit=100):\n    \"\"\"\n    Convert sparse similarity matrix to DataFrame of matches.\n\n    Args:\n        similarity_matrix (sparse matrix): Matrix of similarity scores\n        names_a (list): List of names from first dataset\n        names_b (list): List of names from second dataset\n        limit (int): Maximum number of matches to return\n\n    Returns:\n        pandas.DataFrame: DataFrame containing match pairs and similarity scores\n    \"\"\"\n    # Get non-zero elements\n    row_indices, col_indices = similarity_matrix.nonzero()\n\n    # Determine number of matches to process\n    match_count = min(limit, col_indices.size) if limit else col_indices.size\n\n    # Initialize arrays\n    source_names = np.empty([match_count], dtype=object)\n    target_names = np.empty([match_count], dtype=object)\n    scores = np.zeros(match_count)\n\n    # Populate match data\n    for idx in range(match_count):\n        source_names[idx] = names_a[row_indices[idx]]\n        target_names[idx] = names_b[col_indices[idx]]\n        scores[idx] = similarity_matrix.data[idx]\n\n    # Create and return DataFrame\n    return pd.DataFrame({\n        'left_side': source_names,\n        'right_side': target_names,\n        'similarity': scores\n    })\n\n# Main Blocking and Matching Functions\ndef blocking_tf_idf(df_source, df_target, block_vars, block_values, name_column, source_id, target_id,\n                    tfidf_matrix, vectorizer_type='word', top_matches=100, threshold=0.9, debug=0):\n    \"\"\"\n    Perform TF-IDF based blocking and matching on data blocks.\n\n    Args:\n        df_source (DataFrame): Source dataset block\n        df_target (DataFrame): Target dataset block\n        block_vars (list): Variables used for blocking\n        block_values (list): Values for block variables\n        name_column (str): Column name containing company names\n        source_id (str): ID column name in source dataset\n        target_id (str): ID column name in target dataset\n        tfidf_matrix (sparse matrix): Pre-computed TF-IDF matrix\n        vectorizer_type (str): Type of vectorizer ('word' or 'char')\n        top_matches (int): Number of top matches to consider\n        threshold (float): Similarity threshold for matching\n        debug (int): Debug level flag\n\n    Returns:\n        tuple: (match_rate, block_size, matched_data, combinations, unmatched_data)\n    \"\"\"\n    if debug == 1:\n        print(f'Source block size: {len(df_source)}')\n        print(f'Target block size: {len(df_target)}')\n\n    if len(df_source) &gt; 0 and len(df_target) &gt; 0:\n        # Extract relevant data\n        source_indices = df_source['temp_nameid'].tolist()\n        source_names = df_source[name_column].tolist()\n        source_tfidf = tfidf_matrix[source_indices]\n\n        target_indices = df_target['temp_nameid'].tolist()\n        target_names = df_target[name_column].tolist()\n        target_tfidf = tfidf_matrix[target_indices]\n\n        # Calculate similarities\n        similarity_matrix = cos_sim_top(source_tfidf, target_tfidf.transpose(), \n                                            top_matches, threshold)\n        matches_df = get_matches_dataframe(similarity_matrix, source_names, target_names, \n                                         similarity_matrix.nonzero()[1].size)\n\n        # Select best matches\n        np.random.seed(42)\n        matches_df['random'] = np.random.normal(size=len(matches_df))\n        best_matches = (matches_df.sort_values(['left_side', 'similarity', 'random'], \n                                            ascending=[False, False, False])\n                                .drop_duplicates(['left_side'], keep='first'))\n\n        # Standardize case\n        best_matches['left_side'] = best_matches['left_side'].str.upper()\n        best_matches['right_side'] = best_matches['right_side'].str.upper()\n\n        # Create final datasets\n        matched_records = pd.merge(df_source, best_matches, \n                                left_on=[name_column], right_on=['left_side'], \n                                how='inner')\n        matched_combinations = pd.merge(matched_records, \n                                    df_target[[target_id, name_column]], \n                                    left_on=['right_side'], \n                                    right_on=[name_column], \n                                    how='inner')\n\n        # Handle modified names\n        if name_column.endswith('_mod'):\n            matched_records = process_modified_names(matched_records, df_source, \n                                                  df_target, name_column)\n\n        match_rate = len(matched_records) / len(df_source)\n\n    else:\n        matched_records, matched_combinations = create_empty_blocks(df_source, \n                                                                name_column, \n                                                                source_id, \n                                                                target_id)\n        match_rate = np.NaN\n\n    unmatched_records = get_unmatched_records(df_source, matched_records, source_id)\n\n    return match_rate, len(df_source), matched_records, matched_combinations, unmatched_records\n\n\ndef run_blocking_and_matching(output_dir, data_dir, source_path, target_path, temp_dir, \n                            blocking_variables, name_column, source_id, target_id, \n                            vectorizer_type='word', top_matches=100, threshold=0.9,\n                            additional_columns=[], debug=0):\n    \"\"\"\n    Execute blocking and TF-IDF matching process on datasets.\n\n    Args:\n        output_dir (str): Directory for output files\n        data_dir (str): Directory containing data files\n        source_path (str): Path to source dataset\n        target_path (str): Path to target dataset\n        temp_dir (str): Directory for temporary files\n        blocking_variables (list): Variables used for blocking\n        name_column (str): Column containing names to match\n        source_id (str): ID column in source dataset\n        target_id (str): ID column in target dataset\n        vectorizer_type (str): Type of vectorizer ('word' or 'char')\n        top_matches (int): Number of top matches to consider\n        threshold (float): Similarity threshold for matching\n        additional_columns (list): Extra columns to include\n        debug (int): Debug level flag\n\n    Returns:\n        str: Status indication\n    \"\"\"\n    # Load and preprocess datasets\n    source_data = import_and_preprocess_pa(source_path)\n    target_data = import_and_preprocess_crsp(target_path)\n\n    # Initialize result dictionaries\n    stats = {}\n    counts = {}\n    matched_records = {}\n    match_combinations = {}\n    unmatched_records = {}\n\n    # Generate TF-IDF matrix for all records\n    tfidf_matrix = create_tfidf_matrix(source_data, target_data, name_column, vectorizer_type)\n\n    # Process data blocks and collect results\n    process_blocks(source_data, target_data, blocking_variables, name_column, \n                  source_id, target_id, tfidf_matrix, vectorizer_type,\n                  top_matches, threshold, debug, stats, counts, \n                  matched_records, match_combinations, unmatched_records)\n\n    # Compile and format results\n    summary_statistics = format_statistics(stats, counts)\n    final_matches = pd.concat(matched_records.values())\n    final_combinations = format_combinations(match_combinations, source_data, \n                                          target_data, source_id, target_id)\n\n    # Save processed results\n    save_results(output_dir, blocking_variables, vectorizer_type, final_combinations)\n\n    return 'completed'\n</code></pre>"},{"location":"generated/gallery/","title":"README","text":""},{"location":"generated/gallery/#readme","title":"README","text":"<p>:fontawesome-solid-download: Download all examples in Python source code: gallery_python.zip</p> <p>:fontawesome-solid-download: Download all examples in Jupyter notebooks: gallery_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"}]}